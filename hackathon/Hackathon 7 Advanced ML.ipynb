{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f88f0bd",
   "metadata": {},
   "source": [
    "# Hackathon 7: Advanced ML, Diffusion and Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfff372",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Title: Chaos, Transformers, and Attention\n",
    "\n",
    "*Abstract:* \n",
    "The **attention mechanism** forms the core of the Transformer architecture that undergirds LLMs. It learns to dynamically weight which inputs matter most for a given prediction. Unlike fixed convolutional or recurrent patterns, attention *learns* where to look.\n",
    "\n",
    "In this hackathon, you will build a self-attention layer from scratch and apply it to forecast trajectories of the **Lorenz system**, which we encountered in an earlier hackathon. The Lorenz equations describe simplified atmospheric convection:\n",
    "\n",
    "$$\\frac{dx}{dt} = \\sigma(y - x)$$\n",
    "$$\\frac{dy}{dt} = x(\\rho - z) - y$$\n",
    "$$\\frac{dz}{dt} = xy - \\beta z$$\n",
    "\n",
    "With parameters $\\sigma = 10$, $\\rho = 28$, $\\beta = 8/3$, the system exhibits **chaotic behavior**: tiny differences in initial conditions lead to vastly different trajectories (the \"butterfly effect\").\n",
    "\n",
    "Tne goal of this hackathon is to learn time-series prediction using attention, and to visualize what the attention mechanism learns about the chaotic dynamics of the Lorenz system. The basic steps are:\n",
    "1. **Generate** Lorenz system trajectories.\n",
    "2. **Implement** scaled dot-product attention from scratch.\n",
    "3. **Build** a simple attention-based predictor using self-attention.\n",
    "4. **Train** the model to forecast future states. Study time-series predictions.\n",
    "5. **Visualize** attention weights to interpret what the model focuses on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3cacc6",
   "metadata": {},
   "source": [
    "# Title: Noise, Diffusion, and Phase Transitions\n",
    "\n",
    "*Abstract:*\n",
    "**Diffusion models** have emerged as the dominant generative paradigm behind modern image and video synthesis. The core idea is deceptively simple: systematically destroy structure by adding noise, then train a neural network to reverse the process. Generation becomes iterative denoising.\n",
    "\n",
    "In this hackathon, you will build a diffusion model from scratch and apply it to generate equilibrium configurations of the **Ising model**, which we have now studied extensively. At low temperatures, spins align into ordered domains; at high temperatures, thermal fluctuations dominate and configurations appear random. The transition between these phases is a classic example of symmetry breaking.\n",
    "\n",
    "The goal of this hackathon is to learn generative modeling using diffusion, and to see whether a neural network can capture the statistical structure of a physical system. The basic steps are:\n",
    "\n",
    "1. **Generate** training data: equilibrium Ising configurations via Monte Carlo sampling. We did this in class.\n",
    "2. **Implement** the forward diffusion process that gradually corrupts spin configurations into noise.\n",
    "3. **Build** a simple denoising network that predicts the noise added at each timestep.\n",
    "4. **Train** the model to reverse diffusion. Generate new spin configurations from pure noise.\n",
    "5. **Evaluate** whether generated samples reproduce physical observables: magnetization, correlation functions, domain structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1756cc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "COMP2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
