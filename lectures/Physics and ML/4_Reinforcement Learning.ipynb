{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37244cd7",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "Today's topic, Reinforcement Learning, is closed to what you might have traditionally meant by \"artificial intelligence,\" which maybe isn't a word that you would necessarily apply to a CNN that labels pictures of galaxies (though maybe you would.) When we thing of artificial intelligence, we think of a computer that can learn to play chess, or a robot that can learn to walk. Reinforcement learning is the field of machine learning that deals with these kinds of problems.\n",
    "\n",
    "The essential ideas of RL are:\n",
    "- an *agent* is exploring an *environment.*\n",
    "- at any given time, the agent perceives a *state* of the environment, $s\\in S$.\n",
    "- the agent can take *actions* that change the state of the environment, $a\\in A$.\n",
    "- but how does the agent choose? via a *policy function* $\\pi:S\\rightarrow A$. The policy might be deterministic, or it might be stochastic, an $s$-dependent probability density on $A$.\n",
    "- by following the policy, the agent generates a trajectory through state space, that terminates either at some fixed timeout or when the agent reaches a *terminal state,* characterized by an end goal like checkmate.\n",
    "- at each time step, the agent receives a *reward* $r_t$ that depends on the state, which accumulate into a *return*\n",
    "    $$G_t = \\sum_{k=0}^\\infty \\gamma^k r_{t+k+1}$$\n",
    "    where $\\gamma\\in[0,1]$ is a *discount factor* that determines how much the agent cares about the future.\n",
    "- the expected return given a state is the *value function* a.k.a. *state value function* of a state, \n",
    "        $$v_\\pi(s) = \\mathbb{E}_\\pi[G_t|S_t=s],$$\n",
    "  and the expected return given a state-action pair is the *action value function*,\n",
    "        $$q_\\pi(s,a) = \\mathbb{E}_\\pi[G_t|S_t=s,A_t=a].$$\n",
    "\n",
    "The goal of RL is to find a policy that maximizes the expected return. Some algorithms do this by estimating the value function or action-value function, and then using it to find the optimal policy. These are called *value-based* algorithms. Others directly estimate the optimal policy. These are called *policy-based* methods, or policy gradients.\n",
    "\n",
    "As a litmus test for whether RL is right for your problem, you should ask whether it naturally admits a description in terms of an agent exploring an environment, and taking particular actions to do so. Also necessary is a notion of what is \"good\" or \"bad\" in the environment, to set up your reward. If the state space is small enough, the problem can be solved by going through all the states brute-force. However, in cases with exponentially large state spaces, such as Chess or Go or String Theory, we can't go through all the states and instead use techniques that approximate the value function or policy. In practice, those techniques are neural networks, and the associated RL techniques are called *deep reinforcement learning.*\n",
    "\n",
    "For more on RL, see:\n",
    "- [Sutton and Barto](http://incompleteideas.net/book/the-book-2nd.html), a famous early textbook.\n",
    "- [David Silver's course](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html), a more modern take. The videos were my entry point into RL.\n",
    "- [AlphaZero](https://www.nature.com/articles/nature24270), a 2017 breakthrough in which RL achieves superhuman gameplay in Go *without human knowledge*, i.e. only via knowledge of the game and self-play. It was extended to Chess and Shogi in 2018. See the arXiv article here: [Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm](https://arxiv.org/abs/1712.01815).\n",
    "- My friends and I introduced RL into string theory in [Branes with Brains](https://arxiv.org/abs/1903.11616). We have also used it to [find unknots](https://arxiv.org/abs/2010.16263) and [ribbons](https://arxiv.org/abs/2304.09304), the latter in connection with the smooth 4d PoincarÃ© conjecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld\n",
    "\n",
    "In this class we'll study a famous game in RL called *Gridworld*, see. e.g. Sutton and Barto for more. It's a simple game that is easy to understand, but still has some interesting features. The game is played on a grid, and the agent can move up, down, left, or right. The agent starts in a random position, and the goal is to reach the goal state, which is chosen via the policy.\n",
    "The game ends when the agent reaches the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c19aaa",
   "metadata": {},
   "source": [
    "First we'll set up some helper functions that will be used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c25e224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# get the best action. If several actions are equally good, choose a random one\n",
    "def get_best_action(dct):\n",
    "    best_actions = []\n",
    "    max_key = None\n",
    "    max_val = float('-inf')\n",
    "    for k, v in dct.items():\n",
    "        if v > max_val:\n",
    "            max_val = v\n",
    "            max_key = k\n",
    "            best_actions = [[max_key, max_val]]\n",
    "        elif v == max_val:\n",
    "            best_actions.append([k, v])\n",
    "\n",
    "    return best_actions[np.random.randint(0, len(best_actions))]\n",
    "\n",
    "# randomize the action in 100*eps percent of the cases\n",
    "def random_action(action, action_space, eps=0.3):\n",
    "    p = np.random.random()\n",
    "    if p < (1 - eps):\n",
    "        return action\n",
    "    else:\n",
    "        return np.random.choice(action_space)\n",
    "\n",
    "# Animate the steps taken, ignore this\n",
    "step_counter = 0\n",
    "explore_step = None\n",
    "def animate_steps(agent, window_title, fig_title=\"\"):\n",
    "    plt.ioff()\n",
    "    fig = plt.figure(window_title)\n",
    "    fig.suptitle(fig_title)\n",
    "    mySteps = agent.steps_taken\n",
    "    agent.reset()\n",
    "    step_counter = 0\n",
    "    explore_step = mySteps[step_counter]\n",
    "    im = plt.imshow(agent.render_world(), animated=True)\n",
    "\n",
    "    def update_fig(*args):\n",
    "        nonlocal explore_step, step_counter\n",
    "        if step_counter < len(mySteps):\n",
    "            explore_step = mySteps[step_counter]\n",
    "        else:\n",
    "            step_counter = 0\n",
    "            explore_step = mySteps[step_counter]\n",
    "            agent.reset()\n",
    "        agent.step(explore_step, False)\n",
    "        step_counter += 1\n",
    "        im.set_array(agent.render_world())\n",
    "        plt.draw()\n",
    "        return im,\n",
    "\n",
    "    ani = animation.FuncAnimation(fig, update_fig, interval=150, blit=True, frames=len(mySteps)-1, repeat=True)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(window_title)\n",
    "    # plt.show()\n",
    "    ani.save(\"videos/\"+window_title + \".gif\", writer=animation.PillowWriter(fps=5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec36fc3c",
   "metadata": {},
   "source": [
    "### Defining Gridworld\n",
    "This module defines the Gridworld game environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69aabb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage\n",
    "\n",
    "# The gridworld environment\n",
    "class GameEnv:\n",
    "    def __init__(self):\n",
    "        # initialization of the world\n",
    "        self.sizeX = 9\n",
    "        self.sizeY = 9\n",
    "        self.state = ()  # A state in gridworld is just the (x,y) coordinate pair of the worker\n",
    "        plt.ioff()  # there is currently a bug for Mac users which requires turning this off\n",
    "        self.world_canvas = plt.figure(\"Gridworld\")\n",
    "        self.world_canvas.suptitle('Blue: Worker, Red: Pitfalls, Green: Exit')\n",
    "        self.im = None\n",
    "        plt.axis(\"off\")\n",
    "        self.objects = []\n",
    "        self.initial_x = 0\n",
    "        self.initial_y = 0\n",
    "        self.gave_up = False\n",
    "\n",
    "        # We want the worker to solve the maze as fast as possible without falling into the pits:\n",
    "        # *)   -1 for each step (penalty to solve it quickly)\n",
    "        # *)  -50 for each pitfall (penalty for falling into the pit)\n",
    "        # *) +100 for finding the exit (reward for solving the maze)\n",
    "        # *)   -2 for running into a wall / not moving at all\n",
    "        self.step_penalty = -1.\n",
    "        self.pitfall_penalty = -50.\n",
    "        self.exit_reward = 100.\n",
    "        self.no_move_penalty = -2.\n",
    "\n",
    "        # Actions in gridworld: move up, down, left, right\n",
    "        self.action_space = [0, 1, 2, 3]  # up, down, left, right\n",
    "\n",
    "        # keep track of the total number of steps and the steps that were taken in the game\n",
    "        self.steps = 0\n",
    "        self.steps_taken = []\n",
    "\n",
    "        # maximal number of steps before we give up solving the maze\n",
    "        self.max_steps = 100\n",
    "\n",
    "        # initialize and plot the world\n",
    "        self.world = self.initialize_world()\n",
    "\n",
    "    # initialize a new random world\n",
    "    def initialize_world(self):\n",
    "        self.objects = []\n",
    "\n",
    "        # 1.) The first parameter is the name of the object\n",
    "        # 2.) The second parameter is the reward / penalty:\n",
    "        # 3.) The third parameter is the position of the object in the world\n",
    "        # 4.) Ignore the other parameters, they are just used for drawing the world (box sizes and color)\n",
    "        maze_exit = GameOb('exit', self.exit_reward, self.new_position(), 1, [0, 1, 0, 1])\n",
    "        self.objects.append(maze_exit)\n",
    "        worker = GameOb('worker', None, self.new_position(), 1, [0, 0, 1, 1])\n",
    "        self.objects.append(worker)\n",
    "        for i in range(5):  # add pitfalls\n",
    "            pitfall = GameOb('pitfall', self.pitfall_penalty, self.new_position(), 1, [1, 0, 0, 1])\n",
    "            self.objects.append(pitfall)\n",
    "\n",
    "        # store the initial (x,y) coordinates for a reset\n",
    "        self.initial_x = worker.x\n",
    "        self.initial_y = worker.y\n",
    "\n",
    "        # show the world\n",
    "        world = self.render_world()\n",
    "\n",
    "        # initialize/reset the variables\n",
    "        self.reset()\n",
    "\n",
    "        # plot the world\n",
    "        plt.ioff()\n",
    "        self.im = plt.imshow(world, interpolation=\"nearest\")\n",
    "\n",
    "        return world\n",
    "\n",
    "    # reset the world to its initial configuration, ignore this\n",
    "    def reset(self):\n",
    "        self.steps = 0\n",
    "        self.steps_taken = []\n",
    "        self.gave_up = False\n",
    "        self.state = (self.initial_x, self.initial_y)\n",
    "        for obj in self.objects:\n",
    "            if obj.name == 'worker':\n",
    "                obj.x = self.initial_x\n",
    "                obj.y = self.initial_y\n",
    "                break\n",
    "\n",
    "    # move through the world\n",
    "    # 0 - up, 1 - down, 2 - left, 3 - right\n",
    "    def move_worker(self, direction):\n",
    "\n",
    "        # identify the worker amongst the gridworld objects\n",
    "        worker = None\n",
    "        others = []\n",
    "        for obj in self.objects:\n",
    "            if obj.name == 'worker':\n",
    "                worker = obj\n",
    "            else:\n",
    "                others.append(obj)\n",
    "\n",
    "        worker_x = worker.x\n",
    "        worker_y = worker.y\n",
    "\n",
    "        # overall reward/penalty\n",
    "        reward = self.step_penalty  # penalize each move\n",
    "\n",
    "        # update the position of the worker in gridworld (move if possible)\n",
    "        if direction == 0 and worker.y >= 1:\n",
    "            worker.y -= 1\n",
    "        if direction == 1 and worker.y <= self.sizeY - 2:\n",
    "            worker.y += 1\n",
    "        if direction == 2 and worker.x >= 1:\n",
    "            worker.x -= 1\n",
    "        if direction == 3 and worker.x <= self.sizeX - 2:\n",
    "            worker.x += 1\n",
    "\n",
    "        # move was illegal\n",
    "        if worker.x == worker_x and worker.y == worker_y:\n",
    "            reward = self.no_move_penalty\n",
    "\n",
    "        # update to new position\n",
    "        for i in range(len(self.objects)):\n",
    "            if self.objects[i].name == 'worker':\n",
    "                self.objects[i] = worker\n",
    "                break\n",
    "\n",
    "        # check whether new field is a special field (exit/pitfall) and compute reward/penalty\n",
    "        is_maze_solved = False\n",
    "        for other in others:\n",
    "            if worker.x == other.x and worker.y == other.y:  # the worker ran into an object\n",
    "                if other.name == \"exit\":  # the object was an exit\n",
    "                    is_maze_solved = True\n",
    "                    reward = other.reward\n",
    "                    break  # we can exit the loop here since we can only run into one object\n",
    "                elif other.name == \"pitfall\":  # the object was a pitfall\n",
    "                    is_maze_solved = False\n",
    "                    reward = other.reward\n",
    "                    break   # we can exit the loop here since we can only run into one object\n",
    "\n",
    "        return reward, is_maze_solved\n",
    "\n",
    "    # perform the step, collect the reward, check whether you have reached the exit\n",
    "    def step(self, action, update_view=True):\n",
    "\n",
    "        # collect the reward/punishment for the field the worker ends up in and check whether the exit was reached\n",
    "        reward, done = self.move_worker(action)\n",
    "\n",
    "        self.steps += 1\n",
    "        self.steps_taken.append(action)\n",
    "\n",
    "        # give up\n",
    "        if self.steps >= self.max_steps and not done:\n",
    "            done = True\n",
    "            self.gave_up = True\n",
    "\n",
    "        # this just updates the graphic output of the world\n",
    "        if update_view:\n",
    "            self.im.set_array(self.render_world())\n",
    "            plt.draw()\n",
    "\n",
    "        # return the new state, the penalty/reward for the move and whether gridworld is solved/given up on\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "    # get the current state\n",
    "    def get_state(self):\n",
    "        for obj in self.objects:\n",
    "            if obj.name == 'worker':\n",
    "                return (obj.x, obj.y)\n",
    "\n",
    "    # check whether an action is possible, i.e. whether a wall is blocking the way\n",
    "    def is_possible_action(self, action):\n",
    "        is_possible = False\n",
    "        if action == 0 and self.state[1] >= 1:\n",
    "            is_possible = True\n",
    "        if action == 1 and self.state[1] <= self.sizeY - 2:\n",
    "            is_possible = True\n",
    "        if action == 2 and self.state[0] >= 1:\n",
    "            is_possible = True\n",
    "        if action == 3 and self.state[0] <= self.sizeX - 2:\n",
    "            is_possible = True\n",
    "\n",
    "        return is_possible\n",
    "\n",
    "    ####################################################################################################################\n",
    "    # ignore the code from here on, it just draws the world and represents the objects in the game.\n",
    "    ####################################################################################################################\n",
    "    def close_world_display(self):\n",
    "        plt.close(\"Gridworld\")\n",
    "\n",
    "    def new_position(self):\n",
    "        iterables = [range(self.sizeX), range(self.sizeY)]\n",
    "        points = []\n",
    "        for t in itertools.product(*iterables):\n",
    "            points.append(t)\n",
    "        current_position = []\n",
    "        for objectA in self.objects:\n",
    "            if (objectA.x, objectA.y) not in current_position:\n",
    "                current_position.append((objectA.x, objectA.y))\n",
    "        for pos in current_position:\n",
    "            points.remove(pos)\n",
    "        location = np.random.choice(range(len(points)), replace=False)\n",
    "        return points[location]\n",
    "\n",
    "    def render_world(self):\n",
    "        a = np.zeros([self.sizeY + 2, self.sizeX + 2, 4])\n",
    "        a[0:, 0, 3] = 1  # left wall\n",
    "        a[0, 0:, 3] = 1  # top wall\n",
    "        a[0:, self.sizeX + 1, 3] = 1  # right wall\n",
    "        a[self.sizeY + 1, 0:, 3] = 1  # bottom wall\n",
    "        a[1:-1, 1:-1, :] = 1\n",
    "        for item in self.objects:\n",
    "            if a[item.y + 1, item.x + 1, 0] == 1 and a[item.y + 1, item.x + 1, 1] == 1 and a[item.y + 1, item.x + 1, 2] == 1:  # is completely white\n",
    "                for i in range(len(item.channel)): a[item.y + 1:item.y + item.size + 1, item.x + 1:item.x + item.size + 1, i] = item.channel[i]\n",
    "            else:  # other object on the field, overlay worker with pitfalls / exit\n",
    "                for i in range(len(item.channel)):\n",
    "                    if a[item.y + 1, item.x + 1, i] == 0:\n",
    "                        a[item.y + 1:item.y + item.size + 1, item.x + 1:item.x + item.size + 1, i] += item.channel[i]\n",
    "        a = scipy.ndimage.zoom(a[:, :], (4, 4, 1), order=0, mode=\"nearest\")\n",
    "        return a\n",
    "\n",
    "# This represents an object in the game: worker, pitfall, exit\n",
    "class GameOb:\n",
    "    def __init__(self, name, reward, coordinates, size, RGBA):\n",
    "        self.x = coordinates[0]\n",
    "        self.y = coordinates[1]\n",
    "        self.size = size\n",
    "        self.channel = RGBA\n",
    "        self.reward = reward\n",
    "        self.name = name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea1e76f",
   "metadata": {},
   "source": [
    "Now we have the agent explore Gridworld. Since the state space is small, we can explore the entire environment and find optimal policies. We will do so using an algorithm called *SARSA*. The basic idea is to estimate the action-value function $q_\\pi(s,a)$, and then use it to find the optimal policy. The algorithm is as follows:\n",
    "- Initialize $Q(s,a)$ arbitrarily.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efadae12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I moved up\n",
      "I moved down\n",
      "I moved left\n",
      "I moved right\n",
      "Let the game begin...\n",
      "Watch my exploration route... (close the plot window to continue)\n",
      "Now let me train for a while, I enjoyed the game so much!\n",
      "I'm playing game 0 / 10000\n",
      "I'm playing game 1000 / 10000\n",
      "I'm playing game 2000 / 10000\n",
      "I'm playing game 3000 / 10000\n",
      "I'm playing game 4000 / 10000\n",
      "I'm playing game 5000 / 10000\n",
      "I'm playing game 6000 / 10000\n",
      "I'm playing game 7000 / 10000\n",
      "I'm playing game 8000 / 10000\n",
      "I'm playing game 9000 / 10000\n",
      "Ok, I am done practicing.\n",
      "Watch my exploration route... (close the plot window to continue)\n",
      "Thanks for playing! Bye.\n"
     ]
    }
   ],
   "source": [
    "# Based on a SARSA implementation downloaded from https://github.com/lazyprogrammer/machine_learning_examples/blob/master/rl/sarsa.py\n",
    "# Modified and extended by Fabian Ruehle\n",
    "\n",
    "# Explore Gridworld\n",
    "# import gridworld\n",
    "# import helperFunctions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gridworld:\n",
    "# *) Worker:    Blue\n",
    "# *) Pitfalls:  Red\n",
    "# *) Exit:      Green\n",
    "agent = GameEnv()\n",
    "\n",
    "agent.step(0)\n",
    "print(\"I moved up\")\n",
    "agent.step(1)\n",
    "print(\"I moved down\")\n",
    "agent.step(2)\n",
    "print(\"I moved left\")\n",
    "agent.step(3)\n",
    "print(\"I moved right\")\n",
    "\n",
    "########################################################################################################################\n",
    "# Part 1: Play the game once to see an untrained agent at work\n",
    "########################################################################################################################\n",
    "agent.reset()\n",
    "agent.close_world_display()\n",
    "print(\"Let the game begin...\")\n",
    "\n",
    "# generate all states\n",
    "all_states = []\n",
    "for x in range(agent.sizeX):\n",
    "    for y in range(agent.sizeY):\n",
    "        all_states.append((x, y))\n",
    "\n",
    "# Q is a dictionary that contains the rewards for all four actions that can be performed in any given square of Gridworld.\n",
    "# Initialize Q and keep track of how many times Q[s] has been updated\n",
    "Q = {}\n",
    "update_counts_sa = {}\n",
    "for s in all_states:\n",
    "    update_counts_sa[s] = {}\n",
    "    Q[s] = {}\n",
    "    for a in agent.action_space:\n",
    "        update_counts_sa[s][a] = 1.0\n",
    "        Q[s][a] = 0.0\n",
    "\n",
    "gamma = 0.9  # discount factor\n",
    "alpha_W = 0.1  # learning rate\n",
    "t = 1.0  # count time\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# To start the algorithm, we need any action, so we pick one randomly until we find a valid action which we perform\n",
    "found_initial_move = False\n",
    "current_action = None\n",
    "current_state = agent.get_state()\n",
    "while not found_initial_move:\n",
    "    current_action = random_action(None, agent.action_space, eps=1)\n",
    "    found_initial_move = agent.is_possible_action(current_action)\n",
    "\n",
    "# loop until done (i.e. solved the maze or gave up)\n",
    "done = False\n",
    "while not done:\n",
    "    # perform the current step and get the next state, the reward/penalty for the move, and whether the agent is done (solved or gave up)\n",
    "    next_state, reward, done = agent.step(current_action, False)\n",
    "\n",
    "    # get the best currently known action for the state we are in now\n",
    "    next_action = get_best_action(Q[current_state])[0]\n",
    "    # randomize the action to allow for exploration. As time progresses, make random actions less likely.\n",
    "    next_action = random_action(next_action, agent.action_space, eps=0.4/t)\n",
    "\n",
    "    # Update Q\n",
    "    alpha = alpha_W / update_counts_sa[current_state][current_action]\n",
    "    update_counts_sa[current_state][current_action] += 0.005\n",
    "    Q[current_state][current_action] = Q[current_state][current_action] + alpha * (reward + gamma * Q[next_state][next_action] - Q[current_state][current_action])\n",
    "\n",
    "    # update current state, current action, and start over\n",
    "    current_state = next_state\n",
    "    current_action = next_action\n",
    "    t += 0.001\n",
    "\n",
    "########################################################################################################################\n",
    "# Part 2: Show the exploration route taken by the untrained worker\n",
    "########################################################################################################################\n",
    "\n",
    "# show exploration route\n",
    "result = \"\"\n",
    "if not agent.gave_up:\n",
    "    result = \"I solved gridworld in \" + str(agent.steps) + \" steps.\"\n",
    "else:\n",
    "    result = \"Sorry, I had to give up after \" + str(agent.max_steps) + \" steps.\"\n",
    "\n",
    "# Animate the steps of the first game\n",
    "print(\"Watch my exploration route... (close the plot window to continue)\")\n",
    "animate_steps(agent, \"Gridworld exploration untrained worker\", result)\n",
    "\n",
    "########################################################################################################################\n",
    "# Part 3: Play the game 10,000 times to learn the best solution strategy\n",
    "########################################################################################################################\n",
    "\n",
    "print(\"Now let me train for a while, I enjoyed the game so much!\")\n",
    "\n",
    "agent.reset()\n",
    "plt.close('all')\n",
    "\n",
    "# The code is essentially identical to the one used above, but now carried out 10,000 times\n",
    "training_episodes = 10000\n",
    "for i in range(training_episodes):\n",
    "    if i % 1000 == 0:\n",
    "        print(\"I'm playing game \" + str(i) + \" / \" + str(training_episodes))\n",
    "    if i % 100 == 0:\n",
    "        t += 0.01\n",
    "    agent.reset()\n",
    "    found_initial_move = False\n",
    "    current_action = None\n",
    "    current_state = agent.get_state()\n",
    "    while not found_initial_move:\n",
    "        current_action = random_action(None, agent.action_space, eps=1)\n",
    "        found_initial_move = agent.is_possible_action(current_action)\n",
    "    done = False\n",
    "\n",
    "    # loop until done (i.e. solved the maze or gave up)\n",
    "    while not done:\n",
    "        # perform the current step and get the next state, the reward/penalty for the move, and whether the agent is done (solved or gave up)\n",
    "        next_state, reward, done = agent.step(current_action, False)\n",
    "\n",
    "        # get the best currently known action for the state we are in now\n",
    "        next_action = get_best_action(Q[current_state])[0]\n",
    "        # randomize the action to allow for exploration. As time progresses, make random actions less likely.\n",
    "        next_action = random_action(next_action, agent.action_space, eps=0.4/t)\n",
    "\n",
    "        # Update Q\n",
    "        alpha = alpha_W / update_counts_sa[current_state][current_action]\n",
    "        update_counts_sa[current_state][current_action] += 0.005\n",
    "        Q[current_state][current_action] = Q[current_state][current_action] + alpha * (reward + gamma * Q[next_state][next_action] - Q[current_state][current_action])\n",
    "\n",
    "        # update current state, current action, and start over\n",
    "        current_state = next_state\n",
    "        current_action = next_action\n",
    "\n",
    "print(\"Ok, I am done practicing.\")\n",
    "agent.reset()\n",
    "plt.close('all')\n",
    "\n",
    "########################################################################################################################\n",
    "# Part 4: Show the exploration route taken by the trained worker\n",
    "########################################################################################################################\n",
    "\n",
    "# Navigate the maze using the best steps as learned by the agent\n",
    "current_state = agent.get_state()\n",
    "done = False\n",
    "while not done:\n",
    "    current_action = get_best_action(Q[current_state])[0]\n",
    "    current_state, reward, done = agent.step(current_action, False)\n",
    "\n",
    "result = \"\"\n",
    "if not agent.gave_up:\n",
    "    result = \"I can now solve Gridworld in \" + str(agent.steps) + \" steps.\"\n",
    "else:\n",
    "    result = \"I haven't learned solving Gridworld in \" + str(agent.max_steps) + \" steps.\"\n",
    "\n",
    "# Animate the steps of the trained worker\n",
    "print(\"Watch my exploration route... (close the plot window to continue)\")\n",
    "animate_steps(agent, \"Gridworld exploration trained worker\", result)\n",
    "\n",
    "print(\"Thanks for playing! Bye.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"videos/Gridworld exploration untrained worker.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='videos/Gridworld exploration untrained worker.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"videos/Gridworld exploration trained worker.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='videos/Gridworld exploration trained worker.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5456ee4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This notebook demonstrates the integration and functionality of a Gridworld game environment using Python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
