{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "f35fd598",
            "metadata": {},
            "source": [
                "Numerical Differentiation\n",
                "========================\n",
                "\n",
                "In order to numerically evaluate a derivative $y'(x)=dy/dx$ at point $x_0$, we approximate is by using finite differences:\n",
                "Therefore we find: \n",
                "\n",
                "$dx \\approx \\Delta x =x_1-x_0$\n",
                "\n",
                "$dy \\approx \\Delta y =y_1-y_0= y(x_1)-y(x_0) = y(x_0+\\Delta_x)-y(x_0)$\n",
                "\n",
                "Then we re-write the derivative in terms of discrete differences as:\n",
                "$$\\frac{dy}{dx} \\approx \\frac{\\Delta y}{\\Delta x}$$\n",
                "\n",
                "#### Example\n",
                "\n",
                "Let's look at the accuracy of this approximation in terms of the interval $\\Delta x$. In our first example we will evaluate the derivative of $y=x^2$ at $x=1$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dd88f620",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "fc4875a1",
            "metadata": {},
            "source": [
                "Why is it that the sequence does not converge? This is due to the round-off errors in the representation of the floating point numbers. To see this, we can simply type:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ed2c9e13",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "dc67c872",
            "metadata": {},
            "source": [
                "Let's try using powers of 1/2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ea84faf1",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "ee538f7e",
            "metadata": {},
            "source": [
                "In addition, one could consider the midpoint difference, defined as:\n",
                "$$ dy \\approx \\Delta y = y(x_0+\\frac{\\Delta_x}{2})-y(x_0-\\frac{\\Delta_x}{2}).$$\n",
                "\n",
                "For a more complex function we need to import it from the math module. For instance, let's calculate the derivative of $sin(x)$ at $x=\\pi/4$, including both the forward and midpoint differences."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8158b85f",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "b3604048",
            "metadata": {},
            "source": [
                "What do you notice? Which one does better?\n",
                "\n",
                "A more in-depth discussion about round-off errors in numerical differentiation can be found <a href=\"http://www.uio.no/studier/emner/matnat/math/MAT-INF1100/h10/kompendiet/kap11.pdf\">here</a>"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "80c8b32c",
            "metadata": {},
            "source": [
                "# Autodiff\n",
                "\n",
                "Even better than numerical differentiation is automatic differentiation or *autodiff*, which is crucial to breakthroughs in machine learning.\n",
                "\n",
                "This is a technique that allows to evaluate the derivative of a function to machine precision, without the need to use finite differences, using the fact that autodiff package knows the analytical form of the derivative for certain functions. It then builds a computational graph that allows for the evaluation of the derivative of a function using the chain rule."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f7665212",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7b482490",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "83b976eb",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "af1a8330",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "79639020",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "5e08c47c",
            "metadata": {},
            "source": [
                "Compare to finite difference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ce26ba0e",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "006115b6",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0c049640",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b5963552",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "735c32bc",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "9c0c9fbc",
            "metadata": {},
            "source": [
                "Some of the most complicated functions out there are called *neural networks* which can involve millions or billions of smaller functions or *neurons* in the composition. Autodiff also works on them, which is one of the crucial reasons there has been so much progress in AI in the last 10 years. \n",
                "\n",
                "Let's recall our simple neural network architecture that we studied prevously, a feedforward neural network of depth $3$. The equation defining the network is given by:\n",
                "\n",
                "$$\\mathbf{y} = \\mathbf{W}_3\\sigma(\\mathbf{W}_2\\sigma(\\mathbf{W}_1\\mathbf{x}+\\mathbf{b}_1)+\\mathbf{b}_2)+\\mathbf{b}_3$$\n",
                "\n",
                "where $\\mathbf{x}$ is the input vector, $\\mathbf{y}$ is the neural network prediction, $\\mathbf{W}_i$ and $\\mathbf{b}_i$ are the weight matrices and bias vectors of the network, and $\\sigma$ is the non-linear activation function. Recall that the weights and biases are the parameters that are updated when the network is trained to do something useful.\n",
                "\n",
                "So why is autodiff important here? This function looks complicated, but it's just a composition of affine transformations and elementwise non-linearities. We know the derivative of each of these functions, so we can use the chain rule to compute the derivative of the entire network, which is crucial for training. For functions that are compositions of many smaller functions, this is much more efficient than using finite differences and it has less error."
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3bc6889e",
            "metadata": {},
            "source": [
                "# Ordinary differential equations\n",
                "\n",
                "Let\u2019s consider a simple 1st order equation: \n",
                "\n",
                "$\\frac{dy}{dx}=f(x,y)$\n",
                "\n",
                "To solve this equation with a computer we need to **discretize** the differences: we\n",
                "have to convert the differential equation into a \u201c**finite differences**\u201d equation. The simplest\n",
                "solution is Euler\u2019s method.\n",
                "\n",
                "## Euler\u2019s method\n",
                "\n",
                "Supouse that at a point $x_0$, the function $f$ has a value $y_0$. We\n",
                "want to find the approximate value of $y$ in a point $x_1$ close to\n",
                "$x_0$, $x_1=x_0+\\Delta x$, with $\\Delta x$ small. We assume that $f$,\n",
                "the rate of change of $y$, is constant in this interval $\\Delta x$.\n",
                "Therefore we find: \n",
                "\n",
                "$dx \\approx \\Delta x =x_1-x_0\\\\$\n",
                "$dy \\approx \\Delta y =y_1-y_0$\n",
                " \n",
                "with $y_1=y(x_1)=y(x_0+\\Delta x)$. Then we re-write the differential equation in terms of discrete differences as:\n",
                "\n",
                "$\\frac{\\Delta y}{\\Delta x}=f(x,y)$ \n",
                "\n",
                "or \n",
                "\n",
                "$\\Delta y = f(x,y)\\Delta x$\n",
                "\n",
                "and approximate the value of $y_1$ as\n",
                "$y_1=y_0+f(x_0,y_0)(x_1-x_0)$.\n",
                "\n",
                "We can generalize this formula to find\n",
                "the value of $y$ at $x_2=x_1+\\Delta x$ as\n",
                "\n",
                "$y_{2}=y_1+f(x_1,y_1)\\Delta x,$ \n",
                "\n",
                "or in the general case:\n",
                "\n",
                "$y_{n+1}=y_n+f(x_n,y_n)\\Delta x$\n",
                "\n",
                "This is a good approximation as long as $\\Delta x$ is \u201csmall\u201d. What is\n",
                "small? Depends on the problem, but it is basically defined by the \u201crate\n",
                "of change\u201d, or \u201csmoothness\u201d of $f$. $f(x)$ has to behave smoothly and\n",
                "without rapid variations in the interval $\\Delta x$.\n",
                "\n",
                "Notice that Euler\u2019s method is equivalent to a 1st order Taylor expansion\n",
                "about the point $x_0$. "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "9c36915a",
            "metadata": {},
            "source": [
                "### Exercise 1.1: Newton\u2019s law of cooling \n",
                "\n",
                "If the temperature difference between an object and its surroundings is\n",
                "small, the rate of change of the temperature of the object is\n",
                "proportional to the temperature difference: $\\frac{dT}{dt}=-r(T-T_s),$\n",
                "where $T$ is the temperature of the body, $T_s$ is the temperature of\n",
                "the environment, and $r$ is a \u201ccooling constant\u201d that depends on the\n",
                "heat transfer mechanism, the contact area with the environment and the\n",
                "thermal properties of the body. The minus sign appears because if\n",
                "$T>T_s$, the temperature must decrease.\n",
                "\n",
                "Write a program to calculate the temperature of a body at a time $t$,\n",
                "given the cooling constant $r$ and the temperature of the body at time\n",
                "$t=0$. Plot the results for $r=0.1\\frac{1}{min}$; $T_s=83^{\\circ} C$, $T_0=10^{\\circ}$\n",
                "using different intervals $\\Delta t$ and compare with exact (analytical)\n",
                "results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "98045a53",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5f1b01c6",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "a8fe1b96",
            "metadata": {},
            "source": [
                "Let's try plotting the results. We first need to import the required libraries and methods"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ffa6069d",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8e3bcf55",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "601ffb5f",
            "metadata": {},
            "source": [
                "Alternatively, and in order to re use code in future problems, we could have created a function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e495d383",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7c78a1c4",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "a8b0ee68",
            "metadata": {},
            "source": [
                "Actually, for this particularly simple case, calling a function may introduce unecessary overhead, but it is a an example that we will find useful for future applications. For a simple function like this we could have used a \"lambda\" function (more about lambda functions <a href=\"http://www.secnetix.de/olli/Python/lambda_functions.hawk\">here</a>)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2af2c668",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "a296da95",
            "metadata": {},
            "source": [
                "Now, let's study the effects of different time steps $dt$ on the convergence. We expect that as $dt$ gets small, the result gets better and better but it's slower and slower"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9fdf2bc7",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "22728bf3",
            "metadata": {},
            "source": [
                "We see that the gap between the result gets smaller and smaller as $dt$ gets smaller and smaller. This problem is too simple to see the effect of $dt$ on computation time.\n",
                "\n",
                "But on theoretical grounds, how does computation time scale?"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "c2b4d149",
            "metadata": {},
            "source": [
                "### Challenge 1.1\n",
                "\n",
                "To properly study convergence, one possibility it so look at the result at a given time, for different time steps. Modify the previous program to print the temperature at $t=10$ as a function of $\\Delta t$. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "03ac52dc",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1e6492f4",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "3657d75a",
            "metadata": {},
            "source": [
                "This shows explicit convergence to a fixed value. We'd also like to see the temperature differents"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bbef0863",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "36bb518f",
            "metadata": {},
            "source": [
                "Such a beautiful line on a log-log scale tells us that the temperature difference for different dts scales as a power law. Let's find out what the exponent is by fitting a line to the data.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0a9616a6",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b720b86b",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "id": "3eef8828",
            "metadata": {},
            "source": [
                "The slope being $1$ tells us the power law."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "cc071c98",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}